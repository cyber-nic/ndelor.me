[{"categories":null,"content":"Breathing New Life into Traditional Quebecois Songs with AI In late 2024, I stumbled upon a delightful book titled Chansons du Vieux Québec at the Last Bookshop Jericho in Oxford. Published in 1946, this book is a treasure trove of traditional Quebecois songs, complete with music scores. The idea of using AI to revive these songs struck me as a fascinating project.\nStep 1: Digitizing the Book The first step was to photograph all 250 pages of the book. Armed with my Android phone, I captured each page, and thanks to auto-sync with Google Photos, I quickly transferred the images to my computer. The photos were in HEIC format, Android’s default for saving space, so I converted them to JPEG. Some images needed rotation corrections, which I accomplished using the convert command from the ImageMagick library.\nStep 2: Extracting Text and Lyrics To extract text and song lyrics, I turned to Google Cloud’s Vision API. Using a simple Go script, I iterated over the images, requested OCR responses in JSON format, and saved the results to disk. This process efficiently converted the photographed text into a machine-readable format.\nStep 3: Setting Up AI for Sheet Music Recognition The heart of the project involved converting sheet music to a usable digital format. For this, I leveraged a modest yet capable 2016 NVIDIA GTX 1070 GPU with 8 GB of GDDR5 memory, running CUDA 16.6. After exploring GitHub, I found two promising tools: BreezeWhite/oemer and liebharc/homr . Both tools process sheet music images into MusicXML files, but homr emerged as the better fit because it supports scores with more than two staves per group—a common feature in Chansons du Vieux Québec.\nStep 4: Converting Sheet Music with AI Setting up CUDA and homr was a bit challenging, but I appreciated homr’s use of Python’s Poetry dependency manager, which streamlined the installation. Once configured, processing the sheet music was straightforward. I wrote a simple bash script to process images asynchronously, which completed in a few hours. A nice bonus: homr generates a _teaser.png for each successful detection, overlaying the original image with color-highlighted staff lines.\nStep 5: Creating Audio and Visual Outputs With the MusicXML files ready, I needed to generate audio (MP3) and visual (PNG/PDF) outputs. Enter MuseScore , a free and open-source tool perfect for this task. Another bash script handled the batch conversion efficiently.\nBuilding the Web Experience To present the results, I created a web app hosted on S3 using RemixJS and DaisyUI. A Go script traversed the generated files and produced a JSON representation baked into the app. The UI allows users to browse by chapter, theme, or song, view scanned pages, read extracted text, and listen to audio recordings.\nFuture Directions The project is far from complete. A logical next step is aggregating a song’s MusicXML files into a single file using tools like Relieur. This would allow generating a cohesive MP3 for each song, accommodating musical nuances like repeat signs and first/second endings programmatically.\nAnother exciting prospect involves overlaying the music with AI-generated singing. Research led me to tools like VidnozAI for generating Quebecois-accented speech and kit.ai for converting speech to singing. This idea remains a hypothesis, but it’s an avenue I’m eager to explore.\nConclusion The Chansons du Vieux Québec project has been a rewarding journey, blending traditional culture with modern technology. The current results are available at ndelor.me/projects/quebec . This intersection of history, AI, and creativity has immense potential, and I’m excited about where it might lead next.\nFaire revivre les chansons traditionnelles québécoises grâce à l’IA Fin 2024, j’ai découvert un livre merveilleux intitulé Chansons du Vieux Québec à la librairie Last Bookshop Jericho à Oxford. Publié en 1946, ce livre est un véritable trésor de chansons traditionnelles québécoises accompagnées de partitions musicales. L’idée d’utiliser l’IA pour faire revivre ces chansons m’a semblé être un projet fascinant.\nÉtape 1 : Numériser le livre La première étape consistait à photographier les 250 pages du livre. Munis de mon téléphone Android, j’ai capturé chaque page. Grâce à la synchronisation automatique avec Google Photos, j’ai rapidement transféré les images sur mon ordinateur. Les photos étaient au format HEIC, le format par défaut d’Android pour économiser de l’espace, alors je les ai converties au format JPEG. Certaines images nécessitaient des corrections de rotation, que j’ai réalisées avec la commande convert de la bibliothèque ImageMagick.\nÉtape 2 : Extraire les textes et les paroles Pour extraire les textes et les paroles des chansons, j’ai utilisé l’API Vision de Google Cloud. Avec un script Go simple, j’ai parcouru les images, demandé des réponses OCR au format JSON et enregistré les résultats sur disque. Ce processus a permis de convertir efficacement les textes photographiés en un format lisible par machine.\nÉtape 3 : Configurer l’IA pour la reconnaissance des partitions Le cœur du projet était de convertir les partitions en un format numérique utilisable. Pour cela, j’ai utilisé un GPU NVIDIA GTX 1070 de 2016, modeste mais performant, avec 8 Go de mémoire GDDR5, fonctionnant sous CUDA 16.6. En explorant GitHub, j’ai trouvé deux outils prometteurs : BreezeWhite/oemer et liebharc/homr . Ces outils traitent les images de partitions en fichiers MusicXML, mais homr s’est avéré plus adapté car il prend en charge les partitions contenant plus de deux portées par groupe—une caractéristique fréquente dans Chansons du Vieux Québec.\nÉtape 4 : Conversion des partitions avec l’IA Configurer CUDA et homr n’était pas une mince affaire, mais j’ai apprécié l’utilisation de l’outil de gestion des dépendances Poetry par homr, ce qui a facilité l’installation. Une fois configuré, le traitement des partitions était simple. J’ai écrit un script bash pour traiter les images de manière asynchrone, terminant en quelques heures. Un bonus : homr génère un fichier _teaser.png pour chaque détection réussie, superposant des lignes de portées colorées à l’image originale.\nÉtape 5 : Créer des sorties audio et visuelles Avec les fichiers MusicXML prêts, il fallait générer des sorties audio (MP3) et visuelles (PNG/PDF). J’ai découvert MuseScore , un outil gratuit et open-source parfait pour cette tâche. Un autre script bash a permis d’effectuer la conversion par lots sans problème.\nConstruire l’expérience web Pour présenter les résultats, j’ai créé une application web hébergée sur S3 utilisant RemixJS et DaisyUI. Un script Go a parcouru les fichiers générés et produit une représentation JSON intégrée à l’application. L’interface utilisateur permet de naviguer par chapitre, thème ou chanson, de visualiser les pages scannées, de lire le texte extrait et d’écouter les enregistrements audio.\nPerspectives d’avenir Le projet est loin d’être terminé. Une étape logique serait d’agréger les fichiers MusicXML d’une chanson en un seul fichier à l’aide d’outils comme Relieur. Cela permettrait de générer un MP3 cohérent pour chaque chanson, en intégrant des nuances musicales comme les signes de reprise et les premières/deuxièmes fins de manière programmatique.\nUne autre perspective passionnante est de superposer la musique avec des voix générées par l’IA. Mes recherches m’ont conduit à des outils comme VidnozAI pour générer un discours avec un accent québécois, et kit.ai pour transformer le discours en chant. Cette idée reste une hypothèse, mais c’est une piste que j’ai hâte d’explorer.\nConclusion Le projet Chansons du Vieux Québec a été une aventure enrichissante, mélangeant culture traditionnelle et technologie moderne. Les résultats actuels sont disponibles sur ndelor.me/projects/quebec . L’intersection entre histoire, IA et créativité offre un potentiel immense, et je suis enthousiaste quant à l’avenir de ce projet.\n","permalink":"http://localhost:1313/posts/ai-quebec/","tags":["music","ai","projects"],"title":"AI - Chansons du Vieux Québec"},{"categories":null,"content":"Recently I wanted to reverse engineer an application that uses websockets to communicate with with its server. I wanted to see the data that was being sent and received. I used mitmproxy to intercept the traffic and view it in plain text.\nWebSocket traffic operates over TCP, initially using HTTP for handshake (usually port 80 for unencrypted or 443 for encrypted connections). After the handshake, it establishes a persistent, full-duplex communication channel over the same TCP connection.\nCapturing WebSocket traffic can be challenging. This post explores two approaches:\nDirect approach: Traffic → Local Proxy → Internet Using Redsocks: Traffic → Redsocks → Local Proxy → Internet My key challenges ended up being:\nTrusting mitm\u0026rsquo;s self-signed cert Forcing websocket traffic through local proxy This post is tailored to Ubuntu 24.04\nFinally, it is important to remember that this is a fully functional hack, not a production solution. While mitmproxy supports intercepting and proxying websocket traffic our goal is to view thew content of the messages exchanged via websocket. To achieve this aim the websocket-simple.py addon was used to simply publish message content to the mitm event log. mitmdump is the used to save this to file.\nLocal Proxy Options Local Proxy are interactive, SSL/TLS-capable intercepting applications. They allow to intercept, inspect, modify, and replay web traffic. It is commonly used for debugging, testing, privacy measurements, and penetration testing. Mitmproxy can handle HTTP, HTTPS, and WebSocket traffic, making it a versatile tool for analyzing and manipulating network communications.\nOver the years I\u0026rsquo;ve used several lightweight local proxies. While this post will use mitmproxy, the steps should be similar with other tools (that support websockets).\nFeature mitmproxy Charles Fiddler Platform Cross-platform (CLI/GUI) Cross-platform (macOS, Windows) Cross-platform (Windows, macOS) Open Source Yes No No Interface CLI and web-based GUI GUI-focused GUI-focused Ease of Use Medium (CLI requires learning) High (intuitive GUI) High (intuitive GUI) Protocol Support HTTP/HTTPS, WebSocket HTTP/HTTPS HTTP/HTTPS, WebSocket Advanced Features Scriptable in Python Extensive session handling Scriptable with .NET Price Free Paid (trial available) Free (Pro version available) This post uses mitmproxy because it is free and works well.\nBasic Setup Installation Download and install mitmproxy from the official website. Avoid installing via apt as the version will be outdated. The installation package will include mitmproxy, mitmweb and mitmdump. In this post we\u0026rsquo;ll use both mitmproxy and mitmdump.\nmitmproxy: Best for terminal enthusiasts who need interactive control over traffic in real time. mitmweb: Suited for users preferring a graphical interface with more visual traffic inspection. mitmdump: Ideal for automated workflows and scripting in environments where a UI is unnecessary. Binaries need to be in a location accessible by the mitmproxy user. Keep reading.\n$ sudo cp mitm* /usr/local/bin/ # Check version $ mitmproxy -v Mitmproxy: 11.0.2 binary Python: 3.12.7 OpenSSL: OpenSSL 3.4.0 22 Oct 2024 Platform: Linux-6.8.0-51-generic-x86_64-with-glibc2.39 Basic Usage The next step for intercepting HTTP/HTTPS traffic is to force traffic through the proxy. This can be done at the network device, os routing (eg. iptables) or application level.\nMany applications honor the http_proxy enviornment variables:\nexport http_proxy=http://127.0.0.1:8080 export https_proxy=https://127.0.0.1:8080 Start mitmproxy or mitmweb and view flowing traffic. For intercepting websocket traffic keep reading.\nCreate User A little further down iptables rules will be created to funnel all outgoing os traffic through our proxy. An extremely important exception will be the local proxy\u0026rsquo;s own traffic. Without this exception an infinit loop is created: the proxy\u0026rsquo;s traffic is funneled back to the proxy. Creatung a user helps avoid this problem.\nIt is highly recommended that mitmproxy run as a dedicated user.\n# create user and user\u0026#39;s home dir sudo useradd -m -s /bin/bash mitmproxy sudo chown -R mitmproxy:mitmproxy /home/mitmproxy/.mitmproxy # become mitmproxy sudo su - mitmproxy # run mitmproxy as mitmproxy in order to generate self-signed certs mitmproxy # (exit) Notes:\nmitmproxy (user) is not being granted permission to sudo as this is unecessary. Keep this in mind as you run below commands. mitmproxy (app) will generate self-signed certs in ~/.mitmproxy on it\u0026rsquo;s first run. Trust Self-Signed Cert This was a major pain on Ubuntu 24.04. While the official instructions say to copy mitmproxy-ca-cert.pem over to /usr/local/share/ca-certificates/mitmproxy.crt this does not work. Instead:\n# convert PEM to CRT openssl x509 -in mitmproxy-ca-cert.cer -inform PEM -out mitmproxy-ca-cert.crt # copy CRT to /usr/share/ca-certificates sudo cp /home/mitmproxy/.mitmproxy/mitmproxy-ca-cert.crt /usr/share/ca-certificates/mitmproxy-ca-cert.crt # notice /usr/share rather then /usr/local/share ^ # add to ca-certificates.conf echo mitmproxy-ca-cert.crt | sudo tee -a /etc/ca-certificates.conf # update ca certs sudo update-ca-certificates You might have to reopen some apps for the trust to take effect. It might still not be enough! In our case we\u0026rsquo;ll also use nodejs\u0026rsquo;s NODE_EXTRA_CA_CERTS env var. Keep reading. Start Proxy As mentioned earlier our goal is to view websocket message content. mitmproxy does not offer this out of the box, but it can be achieved using an addon.\nNote:\nNothing will be displayed until the iptables sectin below is completed Once flowing, websockets show a single entry in the flow log. Hit \u0026lsquo;E\u0026rsquo; to view the Event Log. # become mitmproxy (user) # download websocket-simple.py addon wget https://github.com/mitmproxy/mitmproxy/blob/main/examples/addons/websocket-simple.py # start mitmproxy mitmproxy --showhost -s websocket_simple.py --mode transparent --set websocket=true -v # use mitmdump to save to file mitmdump --showhost -s websocket_simple.py --mode transparent --set websocket=true -v | tee -a eventlog.txt showhost: Use the Host header to construct URLs for display. s websocket_simple.py: Our addon to print websocket messages to the mitm event log mode transparent: This is required when apps are not aware of the proxy \u0026ndash; which is the current case as iptables forces the traffic through the proxy. websocket: optional Enable/disable WebSocket support. WebSocket support is enabled by default. v: verbose OS Routing Configure iptables to route all os traffic to the proxy.\nNote: Rebooting your system will reset routing rules to their original values.\nCreate rules # enable IP forwarding sudo sysctl -w net.ipv4.ip_forward=1 # 1. Exclude localhost traffic sudo iptables -t nat -A OUTPUT -o lo -j ACCEPT # 2. Exclude traffic from mitmproxy\u0026#39;s user sudo iptables -t nat -A OUTPUT -m owner --uid-owner mitmproxy -j ACCEPT # 3. Exclude traffic destined for mitmproxy itself sudo iptables -t nat -A OUTPUT -p tcp --dport 8080 -j ACCEPT # 4. Redirect HTTP/HTTPS traffic to mitmproxy sudo iptables -t nat -A OUTPUT -p tcp --dport 80 -j REDIRECT --to-port 8080 sudo iptables -t nat -A OUTPUT -p tcp --dport 443 -j REDIRECT --to-port 8080 View # quick view sudo iptables -t nat -L -v # view with line numbers -- useful for deleting sudo iptables -t nat -L -v --line-numbers At this point traffic should be flowing and sudo iptables -t nat -L -v | grep REDIRECT should show non-zero pkts bytes.\nReverting Run sudo iptables -t nat -L -v --line-numbers to get the line number, followed by sudo iptables -t nat -D OUTPUT 6 where 6 is the desired rule to remove. Hard reset by rebooting you system. Intercept NodeJS websocket traffic Somewhat surprisingly after everything above NodeJS was still not trusting mitmproxy\u0026rsquo;s self-siged cert. Thankfully NODE_EXTRA_CA_CERTS did the trick:\nexport NODE_EXTRA_CA_CERTS=/usr/share/ca-certificates/mitmproxy-ca-cert.crt Troubleshooting If you encounter issues, check:\nFile descriptor limits: ulimit -n # Increase if needed ulimit -n 65535 Process connections lsof -p $(pidof mitmproxy) Test connection # if mitmproxy has mode=transparent curl -v https://www.ndelor.me/ # if mitmproxy does **NOT** have mode=transparent curl -v --proxy http://127.0.0.1:8080 https://www.ndelor.me/ Optional: Using Redsocks In the end I didn\u0026rsquo;t get this solution to work but am including it here for future reference.\nredsocks – is a transparent TCP-to-proxy redirector that allows you to redirect any TCP connection to SOCKS or HTTPS proxy using your firewall, so redirection may be system-wide or network-wide.\nInstallation sudo apt install redsocks Iptables ## Create new chain sudo iptables -t nat -N REDSOCKS # Ignore LANs and some other reserved addresses. sudo iptables -t nat -A OUTPUT -d 0.0.0.0/8 -j RETURN sudo iptables -t nat -A OUTPUT -d 10.0.0.0/8 -j RETURN sudo iptables -t nat -A OUTPUT -d 100.64.0.0/10 -j RETURN sudo iptables -t nat -A OUTPUT -d 127.0.0.0/8 -j RETURN sudo iptables -t nat -A OUTPUT -d 169.254.0.0/16 -j RETURN sudo iptables -t nat -A OUTPUT -d 172.16.0.0/12 -j RETURN sudo iptables -t nat -A OUTPUT -d 192.168.0.0/16 -j RETURN sudo iptables -t nat -A OUTPUT -d 198.18.0.0/15 -j RETURN sudo iptables -t nat -A OUTPUT -d 224.0.0.0/4 -j RETURN sudo iptables -t nat -A OUTPUT -d 240.0.0.0/4 -j RETURN # Anything else should be redirected to port 12345 sudo iptables -t nat -A OUTPUT -p tcp -j REDIRECT --to-ports 12345 ## dump sudo tcpdump -i lo port 12345 journalctl -u redsocks Configuration Create/edit /etc/redsocks.conf:\nbase { log_debug = off; log_info = on; log = \u0026#34;syslog:daemon\u0026#34;; daemon = on; user = redsocks; group = redsocks; redirector = iptables; } redsocks { local_ip = 127.0.0.1; local_port = 12345; ip = 127.0.0.1; port = 8080; type = http-connect; } redsocks { local_ip = 127.0.0.1; local_port = 12346; ip = 127.0.0.1; port = 8080; type = http-relay; } Start redsocks:\nsudo systemctl restart redsocks sudo systemctl status redsocks # Verify port sudo netstat -tulnp | grep 12345 ","permalink":"http://localhost:1313/posts/mitmproxy-node/","tags":["code"],"title":"Capturing WebSocket Traffic with Mitmproxy"},{"categories":null,"content":"Table of Contents Managing Security and Cost: Accounts as Environments Managing Performance and Risk: Layers and Overlays Managing Code: Repos. Folders, and Pipelines Dedicated Repo for Account and IaC Service Account Setup Application Specific Service Accounts Managing Releases: Applications as Modules Integrated Deployment Structure Dedicated Deployment Structure Isolated Deployment Structure Managing Environments: Using Terraform Workspaces vs Dedicated Folders Terraform Workspaces Dedicated Folders Summary This article delves into the complexities and best practices of structuring Terraform projects, focusing on optimizing infrastructure as code for security, cost management, and performance. It discusses the benefits of using separate AWS accounts for each environment to enhance security and manage costs, and explores different strategies for managing code and deployments through layers and overlays. The article compares the use of Terraform workspaces versus dedicated folders, providing guidance on when each method is most effective based on the size and complexity of the project. Ultimately, it offers practical recommendations on setting up and maintaining scalable, secure, and efficient Terraform infrastructure, making it a valuable resource for developers and teams working with infrastructure as code.\nAbstractions are great, but aren\u0026rsquo;t always necessary: “a little copying is better than a little dependency”\n1. Managing Security and Cost: Accounts as Environments Create an Account for each Environment\nEven if it does present overhead creating an AWS account for each environment offers better cost and security isolation. An AWS account requires a unique email address but this can be managed by using email aliases (eg. my-email+my-dev-account-name@gmail.com). It might even be worth creating Project or Team Accounts, also split into environments (eg. my-dev-eng-shared-resources vs my-prd-eng-shared-resources vs my-prd-stealth-project). Creating many accounts involves overhead, so plan accordingly.\n2. Managing Performance and Risk: Layers and Overlays Manage your IaC in well-crafted layers.\nInfrastructure as code (IaC) it can quickly become complex and involve tradeoffs and risk management when it comes to managing dependencies. Let\u0026rsquo;s distinguish between Configuration Management (CM) and [Code] Deployment. To continuously detect and correct these drifts we want to run our Configuration Management layer(s) frequently. In small projects CM might be applied continuously alongside the application code deployments, but as codebases grow this can become the source of overhead (slowness) and needs to be applied separately. This is the bottom of our code release pyramid. The blast radius is large as mistakes can be of severe consequences.\nApplication code is at the top of the pyramid. Ideally, this code deploys for each commit to a branch.\nThe pyramid can be of variable depth. Typically I operate between 1 and 3 layers:\n/\\ / \\ /app \\ -\u0026gt; App code, runs continuously / code \\ / \\ / vpc + db \\ -\u0026gt; App-specific Base layer, runs regularly / \\ / account setup\\ -\u0026gt; Infra or Core layer, runs periodically or on-demand / and iam \\ Along with Configuration Management and Application Code when writing IaC it is necessary to factor in other dimensions such as Code Repositories (where should the code live?), Environments (dev, prd, etc) and Pipelines (the order in which to apply the various layers). As always the best solution depends on your project and team needs.\n3. Managing Code: Repos. Folders, and Pipelines Set conventions and stick to them.\nHere are a few ideas for organizing IaC layers.\n3.1 Account and Pipeline Service Account Setup Create a dedicated Repo with Environment-Specific Account Setup and IAM\nWhile this involves overhead for a small project, I highly recommend starting with this early on in a project. The idea is to isolate more sensitive IaC that is less likely to drift frequently. In this example, each account has a dedicated folder (eg. my-dev-account-name) along with a bootstrap folder and terraform.tfstate committed to repo. The env-specific main.tf then uses the bootstrapped backend to create the additional Backends necessary for the application overlays in other repos. It also creates tailor made (ie. with minimal permission policies) Service Accounts for those application overlays.\n/pipelines ci.yaml /accounts /my-dev-account-name main.tf /bootstrap main.tf terraform.tfstate /my-prd-account-name main.tf /bootstrap main.tf terraform.tfstate /modules /service-account 3.2 Application Specific Service Account As for application-specific Service Accounts\u0026hellip;\nIAM Service Account Definition in the Same Monorepo Pros Simplified Management: Keeping everything in the same repo simplifies the process. You have a single source of truth, making it easier to manage and reference. Consistent Versioning: The IAM service account definition will evolve alongside the Terraform code, ensuring that any changes to infrastructure requirements can be synchronized with the corresponding IAM policy updates. SEasier Onboarding: New team members only need access to one repository to see the full context of the infrastructure and its associated IAM policies. Cons Tight Coupling: Tying the IAM service account directly to the monorepo may lead to challenges if you later need to reuse or share this IAM configuration across other projects or repos. Potential Security Risks: With IAM policies living alongside other infrastructure code, there’s a higher risk of accidental modifications, which could lead to security issues. Best Use Case: Suitable when you have a tightly coupled environment where infrastructure and IAM policies are closely linked, and where managing everything in a single place is more convenient.\nIAM Service Account Definition in a Dedicated (Account) Repo Pros Separation of Concerns: A dedicated IAM repository keeps identity and access management separate from your infrastructure code, aligning with the principle of least privilege. This reduces the risk of accidental changes and enhances security. Scalability and Flexibility: As your AWS environment grows, this separation allows you to manage IAM policies more flexibly, potentially sharing or reusing them across multiple projects or repos. Security Best Practices: It’s easier to apply stricter access controls and review processes on a dedicated IAM repo, reducing the likelihood of unauthorized changes. Cons Increased Complexity: Requires managing multiple repositories, which can add complexity to your workflow, especially during setup and maintenance. Coordination Overhead: Changes to IAM policies might need to be coordinated with updates in your Terraform infrastructure, leading to possible synchronization challenges. Best Use Case: Ideal for environments that prioritize security, scalability, and flexibility, especially when the IAM service accounts might be shared or reused across different teams or projects.\n4. Managing Releases: Applications as Modules Note that Environment and Account are interchangeable terms in the case of a 1:1 mapping\n4.1 Integrated Deployment Structure /pipelines my-dev-project.yaml my-prd-project.yaml /iac /accounts /my-dev-project state.tf main.tf # calls ../../modules/vpc, ../../modules/database, ../../applications/app1, ../../applications/app2 /my-prd-project state.tf main.tf # calls ../../modules/vpc, ../../modules/database, ../../applications/app1, ../../applications/app2 /modules /vpc /database /applications /app1 main.tf variables.tf outputs.tf /app2 main.tf variables.tf outputs.tf Overview This structure keeps infrastructure and application deployment tightly coupled within the same Terraform configurations (main.tf) in each environment.\nPros Simplicity: Single main.tf handles all resource deployments, making it straightforward to understand and deploy. Unified Management: Simplifies state management as there\u0026rsquo;s a single state file per environment. Changes to infrastructure and applications are applied simultaneously, ensuring consistency. Cons Complex Dependency Management: Changes to applications may require re-deployment of infrastructure or vice versa, increasing risk. Reduced Flexibility: Harder to apply changes to one component (app vs. infra) without affecting the other. Scalability Issues: As the project grows, the main.tf can become unwieldy, making maintenance challenging. Pipeline Implications A single pipeline per environment might handle both infra and application changes. This requires careful planning to ensure that infra changes do not unnecessarily impact application deployments.\n4.2 Dedicated Deployment Structure /pipelines # dev my-dev-project.yaml my-dev-project-apps.yaml # prd my-prd-project.yaml my-prd-project-apps.yaml /iac /accounts /my-dev-project /base state.tf main.tf # calls ../../modules/vpc, ../../modules/database /apps state.tf main.tf # calls ../../applications/app1, ../../applications/app2 /my-prd-project /base state.tf main.tf # calls ../../modules/vpc, ../../modules/database /apps state.tf main.tf # calls ../../applications/app1, ../../applications/app2 /modules ... /applications ... Overview Infrastructure and applications are separated at the top level, but applications are still grouped under a single main.tf within /apps for each environment.\nPros Better Isolation: Infrastructure code is separated from application code, reducing the risk of unintended side effects when deploying applications. Modularity: Allows for independent updates of infrastructure while having a slightly coupled application setup. Cons Moderate Complexity: While infra is isolated, applications still share a common deployment pipeline which could lead to issues if one application needs changes not applicable to others. Dependency Management: Still requires coordination between the infra and app deployments but less so than the Integrated structure. Pipeline Implications Could use separate pipelines for infra and apps or a single pipeline that manages dependencies internally. This offers flexibility but requires more sophisticated CI/CD logic to handle the partial coupling of applications.\n4.3 Isolated Deployment Structure /pipelines # dev my-dev-project.yaml my-dev-project-app1.yaml my-dev-project-app2.yaml # prd my-prd-project.yaml my-prd-project-app1.yaml my-prd-project-app2.yaml /iac /accounts /my-dev-project /base state.tf main.tf # calls ../../modules/vpc, ../../modules/database /apps /app1 state.tf main.tf # calls ../../applications/app1 /app2 state.tf main.tf # calls ../../applications/app2 /my-prd-project /base state.tf main.tf # calls ../../modules/vpc, ../../modules/database /apps /app1 state.tf main.tf # calls ../../applications/app1 /app2 state.tf main.tf # calls ../../applications/app2 /modules ... /applications ... Overview Each application has its own directory under /apps, and calls to specific applications are made individually, providing the highest level of isolation.\nPros High Isolation: Each application can be deployed independently, reducing the deployment risks associated with shared resources. Flexibility: Easier to manage different lifecycle stages for each application, such as different scaling needs or upgrade paths. Granular Control: Changes to one application do not affect others, and rollbacks can be handled per application. Cons Increased Management Overhead: More complex directory structure and possibly more state files to manage. Potential Redundancy: Some effort may be duplicated across applications, such as similar CI/CD steps for each app. Pipeline Implications Each application likely has its own pipeline, which increases the number of pipelines but provides maximum control over deployment and versioning. Requires robust orchestration to manage multiple pipelines efficiently.\nConclusion Choosing between these structures depends on the organization\u0026rsquo;s operational complexity, the interdependencies of applications and infrastructure, and the team\u0026rsquo;s capacity to manage multiple pipelines.\nFor smaller projects or those with tightly coupled infra and apps, the Integrated Deployment Structure may be simplest. For larger teams requiring better separation without full isolation, the Dedicated Deployment Structure provides a balanced approach. For enterprises with complex environments where applications need to operate independently, the Isolated Deployment Structure offers the best control and reduces cross-application risks. And of course, there are many hybrid approaches to consider\u0026hellip;\n5. Managing Environments: Using Terraform Workspaces vs Dedicated Folders Here are pros and cons of using Terraform workspaces versus dedicated folders for deploying to different environments (e.g., dev, tst, prd).\n5.1 Terraform Workspaces Pros Single Codebase: All environments share a single set of Terraform configuration files, reducing code duplication and making it easier to maintain consistency across environments. Simplified Management: Workspaces allow you to switch between environments easily within the same directory, simplifying workflows for deploying infrastructure to different environments. Reduced Repository Complexity: Since all environments are managed in the same directory, your repository remains simpler with fewer directories or files to manage. State Isolation: Each workspace has its own state file, ensuring that changes in one environment don\u0026rsquo;t affect another. Cons Limited Environment Customization: Customizing configurations for each environment (e.g., different resource counts, instance types) can become challenging, as you have to rely on conditionals and variable management within the same set of files. Potential for Mistakes: The risk of accidentally applying changes to the wrong environment increases, especially if developers are not careful when switching workspaces. Poor Scalability for Complex Infrastructures: For complex projects with significantly different environments, managing everything in a single directory with workspaces can become unwieldy. Dependency Management: Managing dependencies between resources across environments (e.g., shared services or resources) can be cumbersome with workspaces, as they don’t naturally support cross-environment resource sharing. Best Use Case Small to Medium Projects: If your environments are very similar and you have a relatively simple infrastructure, workspaces can be a good choice. They simplify management and reduce code duplication, making them well-suited for small to medium-sized projects. Consistent Infrastructure: If the environments do not require significant customization and the main differences can be managed through variables, workspaces offer an efficient way to handle deployments. 5.2 Dedicated Folders Pros Clear Environment Separation: Each environment has its own folder with separate Terraform configurations, making it easier to manage and customize infrastructure per environment. Environment-Specific Customization: You can tailor each environment\u0026rsquo;s infrastructure independently, without needing to add complex conditionals or logic to a single set of configurations. Better Scalability: As the infrastructure grows in complexity, dedicated folders offer better scalability, allowing you to manage environments independently without cluttering a single directory. Reduced Risk of Mistakes: Since environments are separated by directories, the risk of accidentally deploying changes to the wrong environment is minimized. Easier Cross-Environment Dependencies: Dedicated folders can more easily handle cross-environment dependencies by referencing shared modules or remote state files. Cons Code Duplication: There may be some duplication of code across environment directories, which can lead to inconsistencies if not carefully managed. Higher Maintenance Overhead: Managing multiple folders requires more effort in keeping configurations consistent and synchronized across environments. Repository Complexity: More folders and files can lead to a more complex repository structure, making it harder to navigate, especially for new team members. Best Use Case Large or Complex Projects: For large, complex projects where each environment might have different configurations, dedicated folders provide better scalability and clearer separation of concerns. Highly Customized Environments: If each environment requires distinct configurations or if your infrastructure needs to scale significantly, dedicated folders offer more flexibility and easier management of cross-environment dependencies. Multi-Account Strategies: If you are employing a multi-account strategy (e.g., different AWS accounts for dev, staging, and production), dedicated folders align better with the need for distinct environments that are isolated not just logically, but also physically across accounts. ","permalink":"http://localhost:1313/posts/terraform-project-structures/","tags":["terraform","iac","code"],"title":"Terraform Project Structures: Organizing Infrastructure as Code"},{"categories":null,"content":"Context Many interesting projects now require a modern GPU (or M1, but I\u0026rsquo;m not desperate enough to downgrade from Linux to OSX). Below are notes on how to spin up a VM instance with GPU in GCP and run a basic PyTorch workload. I chose Watermark-Removal-Pytorch .\nCost/Performance After studying the available GPU configurations as well as [VM instance pricing][https://cloud.google.com/compute/vm-instance-pricing] I determined that the most affordable Accelerator optimized configuration (~$250/month) is the N1 + nvidia-tesla-t4 (I operated in the europe-west1 region). This was plenty computing-power for my modest needs. This exercice set me back $0.83 USD.\nTerraform Here are the broad strokes:\nresource \u0026#34;google_service_account\u0026#34; \u0026#34;default\u0026#34; { account_id = \u0026#34;gpu-sa\u0026#34; display_name = \u0026#34;Custom SA for VM Instance\u0026#34; } resource \u0026#34;google_compute_disk\u0026#34; \u0026#34;default\u0026#34; { name = \u0026#34;disk-data\u0026#34; type = \u0026#34;pd-standard\u0026#34; zone = \u0026#34;europe-west1-b\u0026#34; size = \u0026#34;25\u0026#34; } # https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/compute_instance resource \u0026#34;google_compute_instance\u0026#34; \u0026#34;default\u0026#34; { name = \u0026#34;gpu\u0026#34; machine_type = \u0026#34;n1-standard-1\u0026#34; zone = \u0026#34;europe-west1-b\u0026#34; boot_disk { initialize_params { image = \u0026#34;debian-cloud/debian-12\u0026#34; } } attached_disk { source = google_compute_disk.default.id device_name = google_compute_disk.default.name } network_interface { network = \u0026#34;default\u0026#34; access_config { network_tier = \u0026#34;STANDARD\u0026#34; // Ephemeral IP } } guest_accelerator { type = \u0026#34;nvidia-tesla-t4\u0026#34; count = 1 } scheduling { on_host_maintenance = \u0026#34;TERMINATE\u0026#34; automatic_restart = true } service_account { # Google recommends custom service accounts that have cloud-platform scope and permissions granted via IAM Roles. email = google_service_account.default.email scopes = [\u0026#34;cloud-platform\u0026#34;] } metadata = { # ssh-keys = \u0026#34;${var.gce_ssh_user}:${file(var.gce_ssh_pub_key_file)}\u0026#34; # startup-script-url = \u0026#34;gs://\u0026lt;bucket\u0026gt;/path/to/file\u0026#34; startup-script = \u0026lt;\u0026lt;-EOF sudo apt -y install tmux git curl vim python3-full python3-distutils python3-pip # possibly require open ssl libs sudo apt install zlib1g zlib1g-dev libssl-dev libbz2-dev libsqlite3-dev # pyenv to install other python versions curl https://pyenv.run | bash # add pyenv instructions to .bashrc # install GPU drivers sudo apt install -y linux-headers-$(uname -r) build-essential software-properties-common pciutils gcc make dkms # follow instuctions: https://cloud.google.com/compute/docs/gpus/install-drivers-gpu EOF } } Notes GCP suggests using existing Deep Learning VM images that have NVIDIA drivers pre-installed, and also include other machine learning applications such as TensorFlow and PyTorch. This seems like a wise choice (next time!). The attached_disk will be necessary as soon as you start installing dependencies to run any workload. I recommend loading the start-up script from a url (as commented out), but left things inline for this article. The start-up script does not work as-is. These are just notes. export z=europe-west1-b gcloud compute ssh --project=foo --zone=$z my_user@instance-1 gcloud compute scp --project=foo --zone=$z \\ --recurse some_data/ my_user@instance-1:/home/my_user Mount Disk Installing all the apt deps, gpu drivers and python deps (eg. PyTorch) requires a certain amount of disk space. Mount a reasonably sized disk to accomodate your needs. See GCP instructions Obvioulsy lazy and just trying to deliver, my disk was mounted at /home/my_user/.cache/ which is where pip deps are located. Unfortunately pip kept failing because of disk space until it was instructed to also use /home/my_user/.cache/tmp/.\nImageMagick \u0026amp; Watermark-Removal-Pytorch Watermark-Removal-Pytorch is a neat little project. Unfortunately it doesn\u0026rsquo;t quite deliver when removing a watermark from a page of text. Nevertheless, here are notes on setting up the environment. ImageMagick was used to auto-crop a very large watermak image to the same dimensions as a watermark\u0026rsquo;ed image.\n# install imagemagick sudo apt -y install libpng-dev libjpeg-dev libtiff-dev imagemagick # deps identified by repeatedly running: # python3 get-pip.py and # `pip install -r requirements.txt` sudo apt -y install sox ffmpeg \\ libcairo2 libcairo2-dev libcairo2-dev \\ pkg-config python3-dev \\ libgirepository1.0-dev # install some other python version pyenv install 3.6 pyenv local 3.6 # clone repo git clone https://github.com/braindotai/Watermark-Removal-Pytorch # mounted disk hack # TMPDIR=/home/my_user/.cache/tmp/ pip install -r requirements.txt pip install -r requirements.txt Remove Watermark I\u0026rsquo;ll eventually write about the actual watermark removal result, but for now, here\u0026rsquo;s the process:\n# first copy foo.jpg and watermark.png over from local using scp cmd provided earlier f=foo.jpg; convert ../watermark.png -gravity SouthWest -crop $(identify -format \u0026#34;%wx%h\\n\u0026#34; $f)+0+0 \u0026#34;${f%.jpg}\\_mask.png\u0026#34; python3 inference.py --image-path ../foo.jpg --mask-path ../foo_mask.png time python3 inference.py \\ --image-path ../foo.jpg \\ --mask-path ../mask.jpg \\ --max-dim 2048 \\ # default is 512 which otherwise results in a smaller image --training-steps 2000 # completd 20 training steps in 11s Completed: 100%|██████████████████████████████████████| 20/20 [00:08\u0026lt;00:00, 2.37it/s, Loss=0.00155] real 0m11.974s # completed 200 training steps in 90s Completed: 100%|███████████████████████████████████| 200/200 [01:24\u0026lt;00:00, 2.36it/s, Loss=0.000156] real 1m28.352s # completd 2000 training steps in 14m Completed: 100%|██████████████████████████████████| 2000/2000 [14:13\u0026lt;00:00, 2.34it/s, Loss=4.85e-6] real 14m16.682s ","permalink":"http://localhost:1313/posts/gcp-gpu-instance/","tags":null,"title":"GCP Instance with NVIDIA Tesla T4"},{"categories":null,"content":"In 2022, I launched www.4ks.io , a recipe editing and forking website. I used React and ViteJS v2 for the front-end and was really impressed with its performance, particularly its quick Time to Interactive (TTI) metrics.\nRecently, I began exploring NextJS, initially with version 13 and then upgrading to Next 14, to integrate server-side rendering (SSR) for better SEO and providing better i18n tooling. The performance with NextJS was on par with ViteJS, but I ran into some challenges, such as duplicate API calls during SSR. I utilized openapi-typescript-codegen for API swagger definition and highly recommended tRPC based a very positive experience.\nDespite the overall satisfaction, I expressed reservations about NextJS\u0026rsquo;s built-in API, feeling it tries to do too much by combining API functionalities with client code and SSR. I also found Auth0/Next tooling well-designed but somewhat restrictive in client-side session token retrieval. Eventually, I adapted to NextJS\u0026rsquo;s approach, where data interactions pass through a client API (BFF) before reaching the main API.\nThe solutions below describe attempts at enabling client-side routing for a given route and subroutes. Specifically, paths /recipe/:id and /recipe/id:/* should serve the same layout with only a portion of the page replaced. The goal is to no rerender the layout and avoid unecessary API calls. This turned out to be harder then expected.\nConsider this page Pâté au Poulet Campagnard ) with the following layout (image collapsed):\nLayouts, Templates, Pages In my initial attempt to create a more single-page application (SPA)-like experience with NextJS, I planned to use the layout component to outline the recipe page and allow the Next App router to handle the rendering of different tabs. However, a significant challenge arose: the layout components don\u0026rsquo;t receive pathname as props. A suggested solution involved using middleware to add the missing data to the NextJS request. But this approach was far from ideal. Not only is it discouraged by the NextJS team (#57762 ), but it also comes with substantial tradeoffs. For instance, using middleware interferes with the client-side caching of rendered server components , a key feature of the layout component.\nRealizing this, I considered using the template component instead, which appeared to be designed differently in terms of caching. However, like the layout, the template component also doesn\u0026rsquo;t receive pathname information.\nThis experience highlighted a potential area for improvement in NextJS, making it more flexible and less opinionated. Ultimately, I decided to explore alternative approaches, moving beyond the constraints I encountered with the layout and template components.\nRewrites The following idea came from Building a single-page application with Next.js and React Router . Using NextJS rewrites in next.config:\nasync rewrites() { return [ { source: \u0026#39;/recipe/:id/(forks|media|settings|versions)\u0026#39;, destination: \u0026#39;/recipe/:id\u0026#39;, }, ]; }, As part of my experiment with NextJS, I implemented a block in next.config to render the recipe/page.js file for all specified paths, while other paths would result in a 404 error. This setup required the construction of client-side routing. Given my limited needs, I decided to build a simple component-based router instead of adding react-router-dom as a dependency.\nInitially, I was quite satisfied with this approach. The local development experience, particularly with Hot Module Replacement (HMR), was smooth and efficient. It allowed for only the changed parts of the page to be re-rendered, which was exactly what I was aiming for. However, this pleasing experience in the development environment did not translate to the production build. In production, navigating between pages resulted in full page reloads, contrary to the partial re-rendering I experienced during development. This discrepancy led to the realization that this strategy did not offer any real advantage in a production environment, as it failed to maintain the SPA-like behavior I had achieved in development.\nBroken Spirit Ultimately, I decided to stop pursuing a solution for client-side rendering of tab changes and adopted the standard NextJS approach. The final performance of the website is satisfactory, roughly equivalent to my initial ViteJS front-end. One significant advantage I noticed with this approach is the clarity of code. Following the documentation\u0026rsquo;s guidance made implementation straightforward.\nHowever, this approach is not without its drawbacks. Notably, it results in additional API calls when navigating between tabs, which could be mitigated if the layout could be cached on the client side.\nConclusion In conclusion, while the final product turned out great, there\u0026rsquo;s a sense of disappointment in not being able to achieve the specific functionality I wanted. I enjoy working with NextJS, but it comes with trade-offs. The framework is well-thought-out and functions effectively, but its rigidity can be limiting. For example, NextJS seems to favor the Backend-For-Frontend (BFF) pattern, but I would have preferred to use my existing API written in Go. The ability to support client-side API calls directly would have been ideal. The necessity to conform to the BFF pattern is a part of NextJS that I find less appealing.\nAdditional Notes Environment Variables The auth0/nextjs-auth0 lib is really great. I did find confusing that NEXT_PUBLIC_AUTH0_PROFILE is in fact a build-time variable while NEXT_PUBLIC_AUTH0_LOGIN was consumed either at buildtime or runtime. NextJS documentation is clear that NEXT_PUBLIC_ variables are buildtime.\nResources NextJS App Router + tRPC: solaldunckel/next-13-app-router-with-trpc as an example. NextJS + Material-UI: https://github.com/mui/material-ui/tree/master/examples/material-ui-nextjs-ts NextJS + Auth0: https://github.com/auth0/nextjs-auth0/blob/main/EXAMPLES.md ","permalink":"http://localhost:1313/posts/next-thoughts/","tags":["nextjs"],"title":"Next Thoughts"},{"categories":null,"content":"Minikube , KinD (Kubernetes in Docker), and k3d (K3s in Docker) are all tools for running Kubernetes clusters locally, primarily for development purposes. My personal experience with all 3 has been very positive.\nFor the last couple of years I\u0026rsquo;ve been operating on Fedora linux and have been keeping up with the latest releases. Originally I used minikube, but switched over to k3d whe DNS issues prevented it from reaching docker hub. A few months back I switched over to KinD out of simple curiosity. All 3 options drain my laptop\u0026rsquo;s battery, so there isn\u0026rsquo;t a clear winner there.\nHere are some key differences, pros, and cons of each:\nMinikube Key Features:\nIt creates a VM or a Docker container to run the Kubernetes cluster. It has various \u0026ldquo;addons\u0026rdquo; which are built-in to provide additional functionality, such as the dashboard. Pros:\nMature: One of the earliest solutions for local Kubernetes development. Driver Choices: It supports various virtualization backends (VirtualBox, HyperKit, KVM, etc.). Addons: Integrated tools and services like the Kubernetes dashboard, monitoring, logging, and more. Flexibility: It can simulate various Kubernetes features like node conditions. Cons:\nOverhead: VM-based solutions can be heavyweight. Complexity: Can be a bit complicated for new users with various configuration options. KinD (Kubernetes in Docker) Key Features:\nUses Docker containers to create Kubernetes \u0026ldquo;nodes\u0026rdquo;. Developed by Kubernetes Special Interest Groups (SIGs) for testing Kubernetes itself. Pros:\nLightweight: Runs entirely within Docker, no VM overhead. Quick Setup: Typically faster setup times than VM-based solutions. Multi-node Clusters: Easy to create multi-node clusters. CI Friendly: Designed for CI/CD environments and Kubernetes conformance tests. Cons:\nNetworking: Docker-in-Docker can occasionally have networking hiccups. Storage: Not designed for complex storage scenarios, which can be limiting if you want to test storage features or solutions. k3d (K3s in Docker) Key Features:\nUses Docker containers to run K3s, a lightweight Kubernetes distribution. Ideal for edge and IoT use cases. Pros:\nUltra Lightweight: K3s is a minimal Kubernetes distribution. Fast Start: Starts quickly, making it excellent for local development. Integrated Tooling: K3s has integrated networking (Flannel), storage (Local-path-provisioner), and load balancing (Traefik) solutions. Great for Edge: Designed for edge and IoT scenarios, so running it locally can mimic these environments well. Cons:\nLimited Features: Some of the advanced Kubernetes features are stripped out in K3s to keep it lean. Different Experience: Because it\u0026rsquo;s a minimal distribution, some behaviors might differ from a full-fledged Kubernetes. Conclusion: The best tool often depends on the user\u0026rsquo;s specific needs:\nMinikube might be suitable for those who want a more \u0026ldquo;full\u0026rdquo; Kubernetes experience, possibly closer to a production-like environment. KinD is great for users looking for a lightweight solution and those doing CI/CD with Kubernetes. k3d is best for those wanting an ultra-lightweight environment, or if you\u0026rsquo;re specifically looking to work with or test the capabilities of K3s. However, it\u0026rsquo;s worth noting that the lines between these tools are blurring. For instance, Minikube now supports a Docker driver, which means it can also run without a VM, much like KinD and k3d.\nAs a final note, all 3 local Kubernetes options are great for development and work especially well with tilt.dev . Tilt is a tool for local development with Kubernetes. It watches your files for edits, rebuilds your Docker images and restarts your pods automatically. It\u0026rsquo;s a great tool for local development and I highly recommend it.\n** this article was written with AI assistance\n","permalink":"http://localhost:1313/posts/dev-k8s-options/","tags":["kubernetes","kind","minikube","k3d"],"title":"Dev K8s Options"},{"categories":null,"content":"Context In an effort to learn more about the Google Cloud Platform, I built and deployed a website using Cloud Run functions, hosted behind a GCP Load Balancer. The performance was great. Even without conducting any performance or benchmark tests, I observed that the website was very responsive globally, as relayed by a family member in Singapore.\nUnfortunately, the costs were not as favorable. The cost of running a single load balancer was about ~$25 per month. It is simply too much for a personal project that is not generating any revenue.\nThis articles discusses the journey to saving $25/month.\nIt adds up The chart below shows the daily cost of running a single load balancer for a few months. ManagedZone - This is a fixed cost. $0.06-$0.07/day. Network Cloud Armor Rule - Another fixed cost. $0.033/day. Network Cloud Armor Policy - Starts at $0.161/day and creeps up to $0.167/day on the chart below. This is a fixed cost. HTTP Load Balancing: Global Forwarding Rule Minimum Service Charge - Renamed to Cloud Load Balancer Forwarding Rule Minimum Global on August 1st. This is a fixed cost. $0.6/day. Network Load Balancing: Forwarding Rule Minimum Service Charge in Virginia - Cost $0.672/day for the two days I let it run completely. Options The journey began with the gruntwork\u0026rsquo;s wonderful GCP load balancer terraform module (link ). By default this creates a global LB which costs ~$0.80/day or ~$25/month.\nThe module was tweaked to support creating a regional LB which was deployed in the us-east-4 (Virginia) region in May 2023. For clarity, this regional LB replaced the global LB. This blew my mind: the cost increase by ~$0.07 a day to ~$0.87/day or ~$27/month. As you can understand this was not the result I anticipated and the change was quickly reverted.\nThere are multiple reddit posts (r/googlecloud) discussing the cost of running a load balancer. The general consensus is that the cost is too high for individual projects. One might point out that there are multiple possible GCP LB configurations. If there is a cheaper option I have not found it.\nAWS to (my wallet\u0026rsquo;s) rescue AWS does have a free tier load balancer but it is only available for 12 months after creating an account. This felt like punting the problem down the road. I wanted a solution that would work for the long term.\nIn comes AWS CloudFront. I will update this post in a few weeks once enough billing data has been gathered, but my experience thus far with CloudFront suggests the cost will be much more manageable. Serving ndelor.me is currently costing ~$0.50/month for Route53, $0 for CloudFronts.\nSetting up the CloudFront distribution posed no challenge. One important lesson was that the behavior url path cannot be removed when forwarded to an origin. As a result my gin gonic API needed to listen on the /api path and my GCP bucket assets needed to be moved into a /static folder to match the /api/* and /static/* behaviors path.\nCloudFront GCP Origins For a Google Cloud Storage bucket the CloudFront origin domain should be storage.googleapis.com and the origin path should be the bucket name. For example, /foo-bar-bucket.\nWhile my webapp behavior seemed to work right out of the box the API origin did not. The API behavior was returning a 404 or 401 error.\nSeveral Cache Policy and Origin Request Policy combinations result in a 404. It\u0026rsquo;s very hard to troubleshoot this as the API isn\u0026rsquo;t actually receiving any traffic and thus there are no logs or metrics.\nThe combination that ended up working is the Managed-CachingDisabled Cache Policy and the AllViewerExceptHostHeader Origin Request Policy.\nGetting CloudFront to forward the Authorization header was slighty more involved then anticipated although it turns out this matter is discussed here and documented here .\nConclusion Navigating cloud infrastructure can be complex and costly, but my switch from GCP Load Balancer to AWS CloudFront underscored the value of research and adaptability. While challenges are inevitable, they often pave the way to more efficient and cost-effective solutions.\nMy personal project website www.4ks.io now costs $25/month less to operate by combining AWS CloudFront and GCP Cloud Run.\n","permalink":"http://localhost:1313/posts/cloud-lb-cost-savings/","tags":["aws","gcp","cloud"],"title":"Cloud Load Balancer Cost Savings"},{"categories":null,"content":"Hasham Ali\u0026rsquo;s How to Use UUID Key Type with Gorm article was terrific for suggesting how to handle using UUID as the ID in gorm. It took a little more fiddling to be able to use the keys in a many-to-many relationship. In the end, it worked by having to explicitly define the join table and the foreign key constraints. Sample code is below.\nimport ( \u0026#34;time\u0026#34; \u0026#34;github.com/google/uuid\u0026#34; \u0026#34;gorm.io/gorm\u0026#34; ) // BaseAttributes contains common columns for all tables. // This will replace the default gorm.Model : https://pkg.go.dev/gorm.io/gorm@v1.25.4#Model // and in this specific case will replace the uint id type with uuid. type BaseAttributes struct { ID uuid.UUID `json:\u0026#34;id\u0026#34; gorm:\u0026#34;type:uuid;primary_key;\u0026#34;` CreatedAt time.Time UpdatedAt time.Time DeletedAt *time.Time `sql:\u0026#34;index\u0026#34;` } // BeforeCreate is a gorm hook to auto-set a UUID at objection creation time. // https://gorm.io/docs/hooks.html func (base *BaseGormUUID) BeforeCreate(tx *gorm.DB) error { tx.Statement.SetColumn(\u0026#34;ID\u0026#34;, uuid.New()) return nil } type User struct { BaseAttributes Email string `json:\u0026#34;email\u0026#34;` Username string `json:\u0026#34;username\u0026#34;` } // Organization is the representation of an organization model. It type Organization struct { BaseAttributes Name string `json:\u0026#34;name\u0026#34;` CreatedBy uuid.UUID `json:\u0026#34;createdBy\u0026#34;` Users []User `json:\u0026#34;users\u0026#34; gorm:\u0026#34;many2many:organization_users\u0026#34;` } // OrganizationUsers is a many-to-many join table between Organization and User. It is created explicitely so as to // help define the foreign key constraints. type OrganizationUsers struct { BaseAttributes Organization Organization OrganizationID uuid.UUID `gorm:\u0026#34;primaryKey\u0026#34;` User User UserID uuid.UUID `gorm:\u0026#34;primaryKey\u0026#34;` } ","permalink":"http://localhost:1313/posts/gorm-uuid-many-to-many/","tags":["go"],"title":"Gorm UUID Many to Many"},{"categories":null,"content":"Both the go-kit/log and rs/zerolog loggers provide a Caller method that returns the caller of the function that called it. This is useful for logging the function name in the log output. This functionality is immensly useful and roused my curiosity as to how it is implemented.\nzerolog logger caller example\nimport \u0026#34;github.com/rs/zerolog\u0026#34; import \u0026#34;github.com/rs/zerolog/log\u0026#34; func main() { log.Logger = log.With().Caller().Logger() // \u0026lt;-- log.Debug().Str(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;).Msg(\u0026#34;This will be logged with a caller\u0026#34;) } go-kit logger caller example\nimport \u0026#34;github.com/go-kit/log\u0026#34; import \u0026#34;github.com/go-kit/log/level\u0026#34; func main() { var logger log.Logger logger = log.NewLogfmtLogger(os.Stdout) logger = log.With(logger, \u0026#34;caller\u0026#34;, log.DefaultCaller) // \u0026lt;-- level.Debug(logger).Log(\u0026#34;msg\u0026#34;, \u0026#34;This will be logged with a caller\u0026#34;, \u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;) } These are code snippets showing how this can be achieved using the runtime package.\nimport \u0026#34;runtime\u0026#34; // Caller returns the caller of the function that called it. func Caller() string { pc := make([]uintptr, 15) n := runtime.Callers(2, pc) frames := runtime.CallersFrames(pc[:n]) frame, _ := frames.Next() return frame.Function } // Trace returns the file, line and function name of the caller func Trace() (string, int, string) { pc := make([]uintptr, 15) n := runtime.Callers(2, pc) frames := runtime.CallersFrames(pc[:n]) frame, _ := frames.Next() return frame.File, frame.Line, frame.Function } This is a more complete example that returns the frame at a specified index. This is useful when you want to log the caller of the function that called the function that called it. See this go playground example for a demonstration.\nfunc getFrame(skipFrames int) runtime.Frame { // We need the frame at index skipFrames+2, since we never want runtime.Callers and getFrame targetFrameIndex := skipFrames + 2 // Set size to targetFrameIndex+2 to ensure we have room for one more caller than we need programCounters := make([]uintptr, targetFrameIndex+2) n := runtime.Callers(0, programCounters) frame := runtime.Frame{Function: \u0026#34;unknown\u0026#34;} if n \u0026gt; 0 { frames := runtime.CallersFrames(programCounters[:n]) for more, frameIndex := true, 0; more \u0026amp;\u0026amp; frameIndex \u0026lt;= targetFrameIndex; frameIndex++ { var frameCandidate runtime.Frame frameCandidate, more = frames.Next() if frameIndex == targetFrameIndex { frame = frameCandidate } } } return frame } caller := getFrame(1).Function ","permalink":"http://localhost:1313/posts/go-runtime-frame/","tags":["go"],"title":"Go Runtime Frames"},{"categories":null,"content":"Scraping Results This table shows some metadata about the images scraped.\nPrefix Size (GB) Images Distinct Images Duplicate Images Duplicate Images % A 12 71077 48126 22951 32.3% B 456 1672477 1667500 4977 0.3% C 48 290248 278891 11357 3.9% D 29 122001 121977 24 0.0% E 29 212701 209391 3310 1.6% F 5 40301 40301 0 0.0% G 0.04 216 215 1 0.5% \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- Total 579 2409021 2366401 42620 0 The scraping process resulted in 2.4M images. Current cost is $0.374/day in storage for 543.35GB. From the looks of it, GCP buckets are smart enough to save on storage by not duplicating images that are identical. This size discrepancy was also a hint that lead to the duplicate analysis.\nA most notable element in the table above is the uneven distribution of images across the various prefixes. Prefix B contains ~69% of images while A contains only 3%.\nAdditionally it was unexpected to find so many duplicate images under some Prefixes. Prefix A clearly stands out.More on this later as there are posible domain-specific reasons for this. While I was originally tempted to filter out diplicates to save on OCR cost, the bigger picture shows that the total duplicate count it actually quite insignificant. The effort will be better spent on other cost-saving strategies.\nThe OCR price of $1.50/1000 pages leads to a first cost estimate of $3,614 USD. Since this amount is significantly higher than the $1000 budget cost-saving measures are required. Since the documentation states that a page is defined as \u0026ldquo;a single side of a sheet of paper\u0026rdquo;, one possible solution is the aggregation of several images into a single \u0026ldquo;page\u0026rdquo;.\nImage Prefix Distribution To better understand the distribution of images across the various prefixes, I created a histogram of the image counts. Data was fetched from google Object handlers then stored in a SQL database (ie. Cockroachdb) while filtering out duplicate images. Eventually the code to filter out was commented out and the focus was put entirely on getting metadata into SQL. This task cost about $1.004 USD in Regional Standard Class A Operations. See CockroachDB Local for information on the database choice and configuration.\ncode available on github My SQL skills are rudimentary at best. Using Bard, then ChatGPT4, I was able to generate the following SQL query to generate the histogram.\nCREATE VIEW PrefixCounts AS SELECT prefix, section, COUNT(name) AS image_count FROM images GROUP BY section, prefix; CREATE VIEW RangesB AS SELECT prefix, image_count, FLOOR(image_count / 10)*10 AS lower_bound FROM PrefixCounts WHERE section = \u0026#39;B\u0026#39; AND image_count \u0026lt; 1000; SELECT lower_bound || \u0026#39;-\u0026#39; || (lower_bound + 9)::TEXT AS image_range, COUNT(DISTINCT prefix) AS num_prefixes, SUM(image_count) AS total_images FROM RangesB GROUP BY lower_bound ORDER BY lower_bound; Exporting to CSV and plotting in a Google Sheet resulted in the following histograms (note that these are for Group B only):\nPrefix Count function of Image Count shows the number of prefixes that have a given number of images. These are grouped by 10s. For example, there are 1117 prefixes that have between 1 and 9 images. There are 494 prefixes that have between 100 and 109 images, and so on.\nImage count by folder size provides the sum of images across all prefixes that have a given number of images. For example, the 1117 prefixes that have between 1 and 9 images contain a total of 5303 images. There are 51574 images contained in the 494 prefixes that have between 100 and 109 images, and so on.\nProjected OCR Costs Using the results above the following cost estimates were obtained. If it is truly possible to aggregate images from a folder into batches of 10 then cost should also be reduced by a factor of 10. This would bring the total cost down to ~$361.35. These estimates exclude aggregating images for prefixes that have 1-9 images.\nPrefix Img Count Est. OCR Est. aggr. OCR A 48126 $106.62 B 1667500 $2,508.72 $257.57 C 278891 $435.37 D 121977 $183.00 E 209391 $319.05 F 40301 $60.45 G 215 $0.32 \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- Total 2366401 $3,613.53 ~$360 ","permalink":"http://localhost:1313/posts/hai-img-distribution/","tags":["history","ai","sql","projects"],"title":"History AI - Image Duplicates and Distribtion"},{"categories":null,"content":"update 2024-01-11 While not central to this article the use of the CRC32 hash in the code below is noticeable. Since writing this article I learned that the CRC32, particularly the CRC32C variant used by Google Cloud Storage (GCS), is optimized for error detection, not as a unique identifier for data. It has a higher probability of collisions (1 in 4.3 billion) compared to more robust algorithms. To overcome these limitations, SHA-256, a more robust hashing algorithm, is recommended. SHA-256 significantly reduces the likelihood of hash collisions, ensuring better uniqueness for data identification.\nOverview I needed a SQL database for a project and decided to try out CockroachDB. Setting up the free tier cluster was amazingly simple. Unfortunately after only about ~200,000 queries the 50M RUs were completely used up. The docs point out an RU is an abstracted metric that represent the size and complexity of requests made to your cluster. The queries were very simply so this did come as a surprise. CockroachDB seems like a good product, but the free tier is in fact quite limited.\nQuery examples Example 1: Create table This query is called once at startup. If the table does not exist it is created. Notice the elegabt handling of the error code.\n// initTable function performs a CockroachDB sql query using pgx. It uses crdbpgx for transaction handling (retries). func initTable(ctx context.Context, tx pgx.Tx) error { // Create the images table // https://www.cockroachlabs.com/docs/stable/create-table#:~:text=Create%20a%20new%20table%20only,.%2C%20of%20the%20new%20table. _, err := tx.Exec(ctx, \u0026#34;CREATE TABLE IF NOT EXISTS images (name STRING PRIMARY KEY, section STRING, prefix STRING, size FLOAT, crc32 OID)\u0026#34;) if err != nil { var pgErr *pgconn.PgError if errors.As(err, \u0026amp;pgErr) { // fmt.Println(pgErr.Message) // =\u0026gt; syntax error at end of input // fmt.Println(pgErr.Code) // =\u0026gt; 42601 if pgErr.Code != \u0026#34;42P07\u0026#34; { return err } } } return nil } // ... func main() { // database client dsn := fmt.Sprintf(\u0026#34;postgresql://%s:%s@%s\u0026#34;, username, password, connectionString) roach, err := pgx.Connect(ctx, dsn) defer roach.Close(ctx) if err != nil { os.Exit(exitCodeErr) } // ... // Set up table err := crdbpgx.ExecuteTx(svc.Context, roach, pgx.TxOptions{}, func(tx pgx.Tx) error { return initTable(svc.Context, tx) }) if err != nil { panic(err) } } Example 2: Get // getImageCount function performs a CockroachDB sql query using pgx. It uses crdbpgx for transaction handling (retries). // The inner function allows to return the count value from the query since crdbpgx does not support returning values other than an error. func getImageCount(ctx context.Context, roach *pgx.Conn, crc32 uint32) (int, error) { // init count count := 0 // check if image exists in database err := crdbpgx.ExecuteTx(ctx, roach, pgx.TxOptions{}, func(tx pgx.Tx) error { inner := func() error { // inner function rows, err := tx.Query(ctx, \u0026#34;SELECT COUNT(*) FROM images WHERE crc32 = $1\u0026#34;, crc32) if err != nil { return err } for rows.Next() { if err := rows.Scan(\u0026amp;count); err != nil { return err } } return nil } return inner() }) if err != nil { return count, err } return count, nil } func main() { // database client // ... // attrs is a gcp bucket object count, err := getImageCount(ctx, roach, attrs.CRC32C) if err != nil { panic(err) } } CockroachDB local As a result of the free tier being insufficent for my needs, and not wanting to refactor code (too much), I decided to try out the local version. This is a single node version that can be run on a local machine. It is not intended for production use, but for my particular exercice it was sufficient. The setup was a little more involved than the cloud version, but still very simple. The following are the steps I took to get it up and running.\nWARNING: This is not intended for production use. It is a single node version that is not secure. It is intended for local development only. Also, security was not a concern for me.\nBelow are notes, not a proper turorial. I am not a CockroachDB expert. I am just documenting the steps I took to get it up and running.\nInstallation instruction download wget https://binaries.cockroachdb.com/cockroach-v23.1.8.linux-amd64.tgz tar xzf cockroach-v23.1.8.linux-amd64.tgz mv cockroach-v23.1.8.linux-amd64/ cockroach setup mkdir certs my-safe-directory ./cockroach/cockroach cert create-ca --certs-dir=certs --ca-key=my-safe-directory/ca.key ./cockroach/cockroach cert create-node localhost $(hostname) --certs-dir=certs --ca-key=my-safe-directory/ca.key ./cockroach/cockroach cert create-client root --certs-dir=certs --ca-key=my-safe-directory/ca.key start https://www.cockroachlabs.com/docs/stable/cockroach-start ./cockroach/cockroach start-single-node --certs-dir=certs --store=node1 --listen-addr=localhost:26257 --http-addr=localhost:8080 ./cockroach/cockroach \\ start-single-node \\ --store=node1 \\ --listen-addr=localhost:26257 \\ --http-addr=localhost:8080 curl -s -o /dev/null -w \u0026#34;%{http_code}\u0026#34; http://localhost:8080/health ls ./node1/logs connect ./cockroach/cockroach sql --certs-dir=certs --host=localhost:26257 create database and user CREATE DATABASE mydb; SHOW databases; USE mydb; CREATE USER foobar WITH PASSWORD \u0026#39;password\u0026#39;; ALTER ROLE foobar WITH PASSWORD \u0026#39;password1\u0026#39;; GRANT ALL ON DATABASE mydb TO foobar WITH GRANT OPTION; SHOW grants; GRANT admin to foobar; ","permalink":"http://localhost:1313/posts/cockroachdb-local/","tags":["go","CockroachDB"],"title":"CockroachDB Local"},{"categories":null,"content":"I have 2,000,000 images which all containt a watermark pattern. This post will explore options for removing the watermarks in order to improve the quality of the OCR operations to follow.\n1) Skipping Watermark Removal The cheapest option in terms of time and resources is to skip watermark removal altogether. This can be done by filtering out the known watermark text from the OCR results. This is the best short-term solution, as it is relatively easy to implement and does not require any additional software.\n2) SAAS Image AI There are a number of SAAS image AI services that can be used to remove watermarks. One such service is watermarkremover.io. This service allows you to upload images and have the watermarks removed automatically. However, the quality of the results can vary, and the cost can be prohibitive for large datasets.\n3) OSS Image AI There are also a number of open-source image AI services that can be used to remove watermarks. One such service is lama-cleaner . This service is free to use and can be easily installed and run. However, the quality of the results can be inconsistent, and the service may not be able to remove all watermarks. Results were disapointing with my dataset.\n4) Custom Watermark Removal Service The most reliable way to remove watermarks is to build a custom watermark removal service. This can be done using a variety of open-source tools, such as OpenCV. However, this option requires the most time and effort, as you will need to develop and test your own code.\nI found examples of gocv examples lacking. While it isn\u0026rsquo;t the solution retained today I plan to loop back and explore more.\nHere is a very basic gocv sample (which clearly doesn\u0026rsquo;t delivery the expected results).\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;gocv.io/x/gocv\u0026#34; ) func main() { // read image and watermark mask i := os.Args[1] m := os.Args[2] // load image img := gocv.IMRead(i, gocv.IMReadGrayScale) if img.Empty() { fmt.Printf(\u0026#34;Error reading image from: %v\\n\u0026#34;, i) return } // load watermark mask mask := gocv.IMRead(m, gocv.IMReadGrayScale) // create output mat out := gocv.NewMat() // Remove the watermark from the image. gocv.Subtract(img, mask, \u0026amp;out) // view images w0 := gocv.NewWindow(\u0026#34;Src\u0026#34;) w1 := gocv.NewWindow(\u0026#34;Dst\u0026#34;) w2 := gocv.NewWindow(\u0026#34;Mask\u0026#34;) for { w0.IMShow(img) w2.IMShow(mask) w1.IMShow(out) if w0.WaitKey(1) \u0026gt;= 0 || w2.WaitKey(1) \u0026gt;= 0 { break } } } ","permalink":"http://localhost:1313/posts/hai-computer-vision/","tags":["history","ai","computer vision","projects"],"title":"History AI - Part IV: Computer Vision"},{"categories":null,"content":"Target The target site is completely free and public. While the site\u0026rsquo;s performance is sufficient it unfortunately isn\u0026rsquo;t well maintained: SSL cert is expired. Luckily the sought after information is available directly via REST calls. No html parsing necessary.\nProcess The scraping process was performed on a Cloud Compute, Regular Performance, $5/month VM on Vultr.com. The attached 120GB block storage was quickly expanded to 500GB, which increased the cost from $3.00/month to $12.50/month. The scraping operation completed in approximately 1 month using tmux.\nThe documents on the target site were stored in a tree structure, with a handful of top-level nodes. Only the leaf nodes contained images. The website itself fetched a few results at a time, but the same pagination mechanism could be used to fetch significantly larger batches.\nThe Scraper I created a scraper using a standard Go service architecture. The scraper was resilient to errors and system or network failures. The base code for the scraper can be found at cyber-nic/go-svc-tpl .\nOnly two scrapers were ever started concurrently. These scrapers targeted different data trees and had network delays of 500 and 1000 milliseconds respectively to limit the strain on the target site.\nThe scraper service was originally designed using a recurring function called for each new node encountered. However, this made it difficult to debug. I eventually changed the design so that each data node was added to a queue using the container/list package. The queue size was one of the main metrics that I tracked, and it often reached 500,000.\ntype ctxQueue struct{} func contextWithQueue(ctx context.Context, q *list.List) context.Context { return context.WithValue(ctx, ctxQueue{}, q) } func queueFromContext(ctx context.Context) *list.List { if q, ok := ctx.Value(ctxQueue{}).(*list.List); ok { return q } panic(\u0026#34;fail to get queue from context\u0026#34;) } It was decided to simply re-created the api folder structure locally as this would allow human use more effective.\nThe downloadFile function has to take into consideration that the SSL certificate is expired.\nfunc downloadFile(URL, fileName string) (string, error) { // Get the response bytes from the url t := \u0026amp;tls.Config{InsecureSkipVerify: true} http.DefaultTransport.(*http.Transport).TLSClientConfig = t response, err := http.Get(URL) if err != nil { return fileName, err } defer response.Body.Close() if response.StatusCode != 200 { return fileName, errors.New(\u0026#34;received non 200 response code\u0026#34;) } // Create a empty file file, err := os.Create(fileName) if err != nil { return fileName, err } defer file.Close() // Write the bytes to the file _, err = io.Copy(file, response.Body) if err != nil { return fileName, err } return fileName, nil } An interesting problem was not knowing if the api would return a leaf or non-leaf node. Different branches were of various depths so it was challenging to predict. Keeping in mind that leaf nodes have an images property while non-leaf nodes have children nodes my solution was to create a single struct with the combined properties of both.\n// Node is the representation of a leaf or non-leaf node returned by the api. type Node struct { // both leaf and non-leaf have ID ID string `json:\u0026#34;id\u0026#34;` // non-leaf Childs []Child `json:\u0026#34;childs\u0026#34;` // leaf only Data []Image `json:\u0026#34;data\u0026#34;` } Using reflection it possible to write a handy HasField function which help identify if a node is a leaf or non-leaf.\n// HasField function returns true if the provided field name exists // as a field on the Node func (n Node) HasField(f string) bool { value := reflect.ValueOf(n) field := value.FieldByName(f) return field.IsValid() } n := Node{ ID: \u0026#34;abc\u0026#34;, Childs: []Images, } n.HasField(\u0026#34;Childs\u0026#34;) // true n.HasField(\u0026#34;Data\u0026#34;) // false Finally, all json returned by the api was saved to disk. This allows for checking for a local copy prior of performing a network call and possibly avoiding it, saving time and resources on both the scraper and the target site.\nThe final tally is over 2,000,000 images of typed and handwritten documents. These were copied from the vultr blockstorage to a GCP bucket.\ngsutil -m rsync -r ./bar gs://hai-foo-source/bar ","permalink":"http://localhost:1313/posts/hai-scraping/","tags":["history","ai","go","projects"],"title":"History AI - Part III: Scraping"},{"categories":null,"content":"In the summer of 2023, I embarked on an ambitious journey to collect an extensive archive of historical images, totaling close to 2 million, from a public online repository. These captivating images capture typed or handwritten accounts of war experiences, predominantly in a single non-English language, although they encompass various other languages as well.\nWhile I won\u0026rsquo;t disclose the name of the archive or share any documents here, I may provide some information in the future. My primary objective is to leverage modern image, text, and data mining technologies to extract valuable insights from these documents. By employing AI, I aim to identify individuals, events, locations, and precise date/time references mentioned in the accounts. Moreover, I strive to uncover meaningful patterns such as relationships and other human interactions within the dataset.\nAs a second order objective, I seek to assess the feasibility of achieving these goals using readily available tools, whether open-source or commercially. This evaluation includes considerations such as the accuracy of the results, overall processing speed and performance, and cost implications. I aim to answer critical questions like, \u0026ldquo;Can this process scale effectively? Is it possible to industrialize this approach? Does it deliver meaningful value?\u0026rdquo;\nFurthermore, this undertaking provides an opportunity for me to explore and learn numerous new technologies. While sharpening my coding skills, particularly in Go(lang), I also delve into the vast realm of AI technologies. This endeavor involves constantly improving my proficiency in cloud computing, along with exploring diverse coding languages and frameworks.\nFinally, I plan to document and share my journey through blogging.\nWelcome to the ndelor.me\n","permalink":"http://localhost:1313/posts/ai-making-history/","tags":["history ai","new project"],"title":"History AI - Part I: AI Making History"},{"categories":null,"content":"Assumptions / Constraints We will operate on a dataset of ~2,000,000 jpeg images / ~500GB The initial budget is $1000. It is expected that this will increase, but the goal is to re-evaluate the budget prior to spending. We will operate using the Google Cloud Platform (GCP) but might explore other cloud offerings when performance or cost become a concern System Design Scraping I\u0026rsquo;ve implemented scrapers using various languages including PowerShell, Node.js, Python, and Go. This scraper will also be implemented using Go. At a very high level, the scraping service outputs jpeg images in a folder structure specific to the site being scraped.\nInput: url string\nOutput: image files on disk or in bucket\nImage PreProcessing One of the challenges faced is that all the images scraped contain a repetitive watermark. To enhance the quality of subsequent text extraction (OCR), the primary objective is to remove the watermark from these images. By doing so, I aim to obtain clearer and more accurate results.\nUpdate (09/10/23): After some analysis I am of the opinion that the work required to remove the watermark or the processing cloud costs would not provide the necessary ROI to justify the effort and exepense. Instead, tests have demonstrated that the OCR process does detect the watermark and as a result it can be filtered out prior to NLP processing.\nUpdate (09/10/23): Deeper analysis of the upcoming OCR costs ($1.50/1000 pages ) have led me to the conclusion that cost-saving measures are required. A possible solution is the aggregation of several images into a single \u0026ldquo;page\u0026rdquo;.\nUpdate (2/11/23): Final conclusion, the idea of combining images into a single page is pointless has GCP\u0026rsquo;s Document AI OCR documentation states that it will auto-detect, and bill accordingly, for multiple pages in a single image. Since the expected cost of OCR\u0026rsquo;ing this many images is ~$5000USD we\u0026rsquo;ve applied for a Google Research grant to cover the costs. Waiting on a response.\nInput: Image (ie. location of)\nOutput: New enhanced image stored in bucket\nOCR and Text Extraction We strive to perform Optical Character Recognition (OCR) and extract text from a diverse range of sources, including handwritten and typed documents. It\u0026rsquo;s important to note that many documents may also contain additional layers of handwritten text, adding complexity to the extraction process.\nInput: Image (ie. location of)\nOutput: Text or JSON\nNLP and NER Using Natural Language Processing (NLP), the objective is to perform Named Entity Recognition (NER). This involves identifying and categorizing named entities such as people, organizations, locations, and dates within the extracted text. By leveraging NLP techniques, we can gain valuable insights from the recognized entities.\nPart of this step will be producing a weighted score for each entity relationship. This would potentially allow us to visually identify the most important entities and relationships using a tool like Gephi .\nInput: Text\nOutput: JSON\nLLM The final step is to leverage the extracted entities and relationships to produce a Large Language Models (LLM). It will be interesting to see if the model is capable of identifying patterns and relationships that are not immediately obvious to the human eye. I\u0026rsquo;m currently considering multiple options for this step, including GPT-4, H2O LLM Studio , along with other more customized solutions (as an opportunity to learn more about LLM fundamentals).\nInput: TBD\nOutput: TBD\nInputs / Outputs This diagram illustrates the potential flow of data from one service to the next.\nInputs in blue and Outputs in red.\n","permalink":"http://localhost:1313/posts/hai-system-breakdown/","tags":["history","ai","system-design","projects"],"title":"History AI - Part II: System Design"},{"categories":null,"content":"","permalink":"http://localhost:1313/manifest.json","tags":null,"title":""},{"categories":null,"content":"","permalink":"http://localhost:1313/search/_index.de/","tags":null,"title":""},{"categories":null,"content":"","permalink":"http://localhost:1313/search/_index.es/","tags":null,"title":""},{"categories":null,"content":"","permalink":"http://localhost:1313/search/_index.fr/","tags":null,"title":""},{"categories":null,"content":"","permalink":"http://localhost:1313/search/_index.hi/","tags":null,"title":""},{"categories":null,"content":"","permalink":"http://localhost:1313/search/_index.jp/","tags":null,"title":""},{"categories":null,"content":"","permalink":"http://localhost:1313/search/_index.nl/","tags":null,"title":""},{"categories":null,"content":"","permalink":"http://localhost:1313/search/_index.pl/","tags":null,"title":""},{"categories":null,"content":"","permalink":"http://localhost:1313/search/_index.ru/","tags":null,"title":""},{"categories":null,"content":"","permalink":"http://localhost:1313/search/_index.zh-cn/","tags":null,"title":""},{"categories":null,"content":"I do dev and devops things. Currently located in Oxford, UK. Find me on linkedin ","permalink":"http://localhost:1313/about/","tags":null,"title":"About"}]